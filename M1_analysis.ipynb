{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9694d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Unified file list: (file_path, snapshot_name, table_name) ---\n",
    "all_files = [\n",
    "    # Assemblies\n",
    "    (r\"data\\PreNexPartAssemblies.csv\",           \"PreNex\",  \"assemblies\"),\n",
    "    (r\"data\\PartAssemblies_25_06_25.csv\",         \"PostNex\", \"assemblies\"),\n",
    "    (r\"data\\PartAssembliesNGD27_30_06_25.csv\",    \"NGD27\",   \"assemblies\"),\n",
    "    (r\"data\\PartAssembliesDammagedPost27.csv\",    \"Post27\",  \"assemblies\"),\n",
    "\n",
    "    # Revisions\n",
    "    (r\"data\\PreNexPartRevisions.csv\",             \"PreNex\",  \"revisions\"),\n",
    "    (r\"data\\PartRevisions_25_06_25.csv\",           \"PostNex\", \"revisions\"),\n",
    "    (r\"data\\PartRevisionsNGD27_30_06_25.csv\",      \"NGD27\",   \"revisions\"),\n",
    "    (r\"data\\PartRevisionsDammagedPost27.csv\",      \"Post27\",  \"revisions\"),\n",
    "\n",
    "    # Materials\n",
    "    (r\"data\\PreNexPartMaterials.csv\",              \"PreNex\",  \"materials\"),\n",
    "    (r\"data\\PartMaterials_25_06_25.csv\",            \"PostNex\", \"materials\"),\n",
    "    (r\"data\\PartMaterialsNGD27_30_06_25.csv\",       \"NGD27\",   \"materials\"),\n",
    "    (r\"data\\PartMaterialsDammagedPost27.csv\",       \"Post27\",  \"materials\"),\n",
    "]\n",
    "\n",
    "# --- Load into nested dictionary: dfs_snapshots[snapshot][table] ---\n",
    "dfs_snapshots = defaultdict(dict)\n",
    "\n",
    "DATE_FORMAT = \"%d/%m/%Y %I:%M:%S %p\"  # dd/mm/yyyy h:mm:ss AM/PM\n",
    "\n",
    "def parse_mixed_dates(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse a series of date strings that may be in either:\n",
    "      - \"%d/%m/%Y %I:%M:%S %p\"  (12h + AM/PM)\n",
    "      - \"%d/%m/%Y %H:%M\"        (24h, no AM/PM)\n",
    "    \"\"\"\n",
    "    # First pass: 12-hour clock with AM/PM\n",
    "    dt = pd.to_datetime(\n",
    "        series,\n",
    "        format=\"%d/%m/%Y %I:%M:%S %p\",\n",
    "        errors=\"coerce\",\n",
    "        dayfirst=True,\n",
    "    )\n",
    "\n",
    "    # Any failures -> try 24-hour clock\n",
    "    mask = dt.isna() & series.notna()\n",
    "    if mask.any():\n",
    "        dt.loc[mask] = pd.to_datetime(\n",
    "            series[mask],\n",
    "            format=\"%d/%m/%Y %H:%M\",\n",
    "            errors=\"coerce\",\n",
    "            dayfirst=True,\n",
    "        )\n",
    "    return dt\n",
    "\n",
    "def load_csv_with_dates(path: str) -> pd.DataFrame:\n",
    "    # 1) Peek header to find date columns\n",
    "    cols = pd.read_csv(path, nrows=0).columns\n",
    "    date_cols = [c for c in cols if c.endswith(\"CreatedDate\")]\n",
    "\n",
    "    # 2) Read all as strings for date cols (faster CSV read)\n",
    "    dtype = {c: pd.StringDtype() for c in date_cols}\n",
    "    df = pd.read_csv(path, dtype=dtype)\n",
    "\n",
    "    # 3) Convert detected date columns using mixed parser\n",
    "    for c in date_cols:\n",
    "        df[c] = parse_mixed_dates(df[c])\n",
    "\n",
    "    return df\n",
    "\n",
    "for path, snapshot, table in all_files:\n",
    "    dfs_snapshots[snapshot][table] = load_csv_with_dates(path)\n",
    "\n",
    "#ast to regular dict if you're done mutating\n",
    "dfs_snapshots = dict(dfs_snapshots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path, snapshot_name, table_name in all_files:\n",
    "    print(dfs_snapshots[snapshot_name][table_name].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4413aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index columns per table\n",
    "index_map = {\n",
    "    \"assemblies\": [\"imaPartID\", \"imaPartRevisionID\", 'imaMethodID', 'imaMethodRevisionID', 'imaMethodAssemblyID'],\n",
    "    \"revisions\": [\"imrPartID\", \"imrPartRevisionID\", \"imrCreatedBy\"],\n",
    "    \"materials\": [\"immPartID\", \"immPartRevisionID\", 'immMethodID', 'immMethodRevisionID', 'immMethodAssemblyID','immMethodMaterialID']\n",
    "}\n",
    "\n",
    "# Loop snapshots and tables\n",
    "for snapshot, tables in dfs_snapshots.items():\n",
    "    for table, df in tables.items():\n",
    "        if table in index_map:\n",
    "            dfs_snapshots[snapshot][table] = df.set_index(index_map[table])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa648d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test check for column names and index names\n",
    "for snapshot, tables in dfs_snapshots.items():\n",
    "    for table, df in tables.items():\n",
    "        if table == 'revisions':\n",
    "            print(dfs_snapshots[snapshot][table].columns.tolist())\n",
    "            print(dfs_snapshots[snapshot][table].index.names)\n",
    "            print(dfs_snapshots[snapshot][table].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#\"report\" (default): don’t change data; just return dupe reports (no diff if dupes exist).\n",
    "#\"sequence\": add a per-key __seq__ to disambiguate multiplicity and proceed.\n",
    "#\"drop_exact\": drop exact duplicate rows (content-identical) before diffing.\n",
    "\n",
    "def _dupe_counts(df, key_cols):\n",
    "    g = df.reset_index()[key_cols].value_counts(sort=False).rename(\"count\")\n",
    "    return g[g > 1].sort_index()\n",
    "\n",
    "def _add_seq_index(df, key_cols, seq_name=\"__seq__\"):\n",
    "    # Keep original order per key; assign 0..n-1\n",
    "    if list(df.index.names) == key_cols:\n",
    "        df = df.reset_index()\n",
    "    df = df.copy()\n",
    "    df[seq_name] = df.groupby(key_cols).cumcount()\n",
    "    return df.set_index(key_cols + [seq_name])\n",
    "\n",
    "def _drop_exact_dupes(df, key_cols):\n",
    "    return df.reset_index().drop_duplicates().set_index(key_cols)\n",
    "\n",
    "def _row_hashes(df):\n",
    "    # Stable, vectorized row hash (handles NaNs)\n",
    "    h = np.zeros(len(df), dtype=np.uint64)\n",
    "    for c in df.columns:\n",
    "        h ^= pd.util.hash_pandas_object(df[c], index=False).to_numpy(dtype=np.uint64, copy=False)\n",
    "    return h\n",
    "\n",
    "def normalize_for_diff(df):\n",
    "    out = df.copy()\n",
    "\n",
    "    # IDs\n",
    "    if \"imrPartID\" in out.columns:\n",
    "        out[\"imrPartID\"] = out[\"imrPartID\"].astype(\"string\").str.strip()\n",
    "\n",
    "    if \"imrPartRevisionID\" in out.columns:\n",
    "        out[\"imrPartRevisionID\"] = (\n",
    "            out[\"imrPartRevisionID\"]\n",
    "            .astype(\"string\")\n",
    "            .str.strip()\n",
    "            .fillna(\"\")  # sentinel for missing revision\n",
    "        )\n",
    "\n",
    "    # Normalize strings\n",
    "    str_cols = out.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "    out[str_cols] = out[str_cols].apply(\n",
    "        lambda s: s.astype(\"string\").str.strip().replace({\"\": pd.NA})\n",
    "    )\n",
    "\n",
    "    # Normalize dates\n",
    "    for c in out.columns:\n",
    "        if c.endswith(\"CreatedDate\"):\n",
    "            out[c] = pd.to_datetime(out[c], errors=\"coerce\").dt.floor(\"min\")\n",
    "\n",
    "    return out\n",
    "    \n",
    "def fast_anycol_diff(\n",
    "    pre,\n",
    "    post,\n",
    "    key_cols,\n",
    "    label_pre=\"PreNex\",\n",
    "    label_post=\"PostNex\",\n",
    "    duplicate_strategy=\"report\",  # 'report' | 'sequence' | 'drop_exact'\n",
    "):\n",
    "    \"\"\"\n",
    "    Fast diff across ANY columns, with duplicate detection/handling.\n",
    "\n",
    "    Returns dict:\n",
    "      - changed_diff, changed_side_by_side, new_rows, removed_rows\n",
    "      - duplicates_pre, duplicates_post, duplicate_count_diff\n",
    "      - meta: info about strategy applied\n",
    "    \"\"\"\n",
    "     # --- normalize snapshots\n",
    "    df_pre = normalize_for_diff(pre)\n",
    "    df_post = normalize_for_diff(post)\n",
    "\n",
    "    # Ensure indexed by key (no copy if already correct)\n",
    "    if list(df_pre.index.names) != key_cols:\n",
    "        df_pre = df_pre.set_index(key_cols, drop=True)\n",
    "    if list(df_post.index.names) != key_cols:\n",
    "        df_post = df_post.set_index(key_cols, drop=True)\n",
    "\n",
    "    # --- Duplicate detection\n",
    "    dup_pre  = _dupe_counts(df_pre,  key_cols)\n",
    "    dup_post = _dupe_counts(df_post, key_cols)\n",
    "\n",
    "    # Differences in multiplicity (including 0→n, n→0)\n",
    "    cnt_pre  = df_pre.reset_index()[key_cols].value_counts(sort=False).rename(\"count_pre\")\n",
    "    cnt_post = df_post.reset_index()[key_cols].value_counts(sort=False).rename(\"count_post\")\n",
    "    duplicate_count_diff = (\n",
    "        cnt_pre.to_frame().join(cnt_post.to_frame(), how=\"outer\").fillna(0).astype(int)\n",
    "    )\n",
    "    duplicate_count_diff = duplicate_count_diff[\n",
    "        (duplicate_count_diff[\"count_pre\"] != 1) | (duplicate_count_diff[\"count_post\"] != 1)\n",
    "    ].sort_index()\n",
    "\n",
    "    # Optionally resolve before diff\n",
    "    resolved = False\n",
    "    if duplicate_strategy == \"sequence\":\n",
    "        df_pre = _add_seq_index(df_pre, key_cols)\n",
    "        df_post = _add_seq_index(df_post, key_cols)\n",
    "        key_cols = df_pre.index.names  # now includes '__seq__' where applied\n",
    "        resolved = True\n",
    "    elif duplicate_strategy == \"drop_exact\":\n",
    "        if not dup_pre.empty:\n",
    "            df_pre = _drop_exact_dupes(df_pre, key_cols)\n",
    "        if not dup_post.empty:\n",
    "            df_post = _drop_exact_dupes(df_post, key_cols)\n",
    "        resolved = True\n",
    "    elif duplicate_strategy != \"report\":\n",
    "        raise ValueError(\"duplicate_strategy must be 'report', 'sequence', or 'drop_exact'\")\n",
    "\n",
    "    # If reporting only and duplicates exist, return reports without diffing\n",
    "    if duplicate_strategy == \"report\" and (not dup_pre.empty or not dup_post.empty):\n",
    "        return {\n",
    "            \"changed_diff\": pd.DataFrame(),\n",
    "            \"changed_side_by_side\": pd.DataFrame(),\n",
    "            \"new_rows\": pd.DataFrame(),\n",
    "            \"removed_rows\": pd.DataFrame(),\n",
    "            \"duplicates_pre\": dup_pre,\n",
    "            \"duplicates_post\": dup_post,\n",
    "            \"duplicate_count_diff\": duplicate_count_diff,\n",
    "            \"meta\": {\"duplicate_strategy\": \"report\", \"diff_performed\": False},\n",
    "        }\n",
    "\n",
    "    # --- Proceed with diff (unique rows per key at this point)\n",
    "    common  = df_pre.index.intersection(df_post.index)\n",
    "    added   = df_post.index.difference(df_pre.index)\n",
    "    removed = df_pre.index.difference(df_post.index)\n",
    "\n",
    "    cols = df_pre.columns.union(df_post.columns)\n",
    "    pre_common  = df_pre.loc[common].reindex(columns=cols)\n",
    "    post_common = df_post.loc[common].reindex(columns=cols)\n",
    "\n",
    "    # Hash rows to find changed keys fast\n",
    "    h_pre  = _row_hashes(pre_common)\n",
    "    h_post = _row_hashes(post_common)\n",
    "    changed_mask = h_pre != h_post\n",
    "    if changed_mask.any():\n",
    "        changed_idx = pre_common.index[changed_mask]\n",
    "        changed_side_by_side = pd.concat(\n",
    "            [\n",
    "                pre_common.loc[changed_idx].add_suffix(f\"_{label_pre}\"),\n",
    "                post_common.loc[changed_idx].add_suffix(f\"_{label_post}\"),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).reset_index()\n",
    "        changed_diff = post_common.loc[changed_idx].compare(\n",
    "            pre_common.loc[changed_idx],\n",
    "            align_axis=1,\n",
    "            keep_equal=False,\n",
    "            result_names=(label_post, label_pre),\n",
    "        )\n",
    "    else:\n",
    "        changed_side_by_side = pd.DataFrame(columns=list(key_cols))\n",
    "        changed_diff = pd.DataFrame()\n",
    "\n",
    "    new_rows     = df_post.loc[added].reset_index()\n",
    "    removed_rows = df_pre.loc[removed].reset_index()\n",
    "\n",
    "    return {\n",
    "        \"changed_diff\": changed_diff,\n",
    "        \"changed_side_by_side\": changed_side_by_side,\n",
    "        \"new_rows\": new_rows,\n",
    "        \"removed_rows\": removed_rows,\n",
    "        \"duplicates_pre\": dup_pre,\n",
    "        \"duplicates_post\": dup_post,\n",
    "        \"duplicate_count_diff\": duplicate_count_diff,\n",
    "        \"meta\": {\"duplicate_strategy\": \"sequence\" if resolved else duplicate_strategy, \"diff_performed\": True},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ['imrPartID','imrPartRevisionID']\n",
    "pre = dfs_snapshots['PreNex']['revisions']\n",
    "post = dfs_snapshots['NGD27']['revisions']\n",
    "def dup_report(df, name):\n",
    "    dups = df.index.duplicated(keep=False)\n",
    "    print(f\"{name}: total={len(df)}, dup_rows={dups.sum()}, dup_keys={df.index[dups].nunique()}\")\n",
    "\n",
    "dup_report(pre,  \"PreNex\")\n",
    "dup_report(post, \"NGD27\")\n",
    "\n",
    "# See some examples\n",
    "#print(pre[pre.index.duplicated(keep=False)].head(10))\n",
    "#print(post[post.index.duplicated(keep=False)].head(10))\n",
    "\n",
    "#print(pre.loc[\"VSTS2MACK01\"])\n",
    "#print(post.loc[\"VSTS2MACK01\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreNex, PostNex, NGD27, Post27    \n",
    "check = 'revisions'\n",
    "pre = dfs_snapshots['PreNex'][check]\n",
    "post = dfs_snapshots['PostNex'][check]\n",
    "#print(pre.head())\n",
    "#print(post.head())\n",
    "out = fast_anycol_diff(pre, post, index_map[check], duplicate_strategy=\"sequence\")\n",
    "# out['changed_diff'], out['changed_side_by_side'], out['new_rows'], out['removed_rows']\n",
    "print(out['changed_side_by_side'].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a29894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreNex, PostNex, NGD27, Post27    \n",
    "check = 'materials'\n",
    "pre = dfs_snapshots['PreNex'][check]\n",
    "post = dfs_snapshots['Post27'][check]\n",
    "#print(pre.head())\n",
    "#print(post.head())\n",
    "out = fast_anycol_diff(pre, post, index_map[check], duplicate_strategy=\"sequence\")\n",
    "# out['changed_diff'], out['changed_side_by_side'], out['new_rows'], out['removed_rows']\n",
    "print(out['removed_rows'].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreNex, PostNex, NGD27, Post27    \n",
    "check = 'assemblies'\n",
    "pre = dfs_snapshots['PreNex'][check]\n",
    "post = dfs_snapshots['Post27'][check]\n",
    "#print(pre.head())\n",
    "#print(post.head())\n",
    "out = fast_anycol_diff(pre, post, index_map[check], duplicate_strategy=\"sequence\")\n",
    "# out['changed_diff'], out['changed_side_by_side'], out['new_rows'], out['removed_rows']\n",
    "print(out['changed_side_by_side'].to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
